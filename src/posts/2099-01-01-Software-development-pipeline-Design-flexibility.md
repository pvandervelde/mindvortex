Title: Software development pipeline - Design flexibilty
Tags:
  - Delivering software
  - Software development pipeline
  - Pipeline design
  - DevOps
---

The [fourth property](Software-development-pipeline-Design-introduction.html) to consider is
*flexibility*, i.e. the ability of the pipeline to be able to be modified or adapted without
too much effort.

A pipeline should be flexible because the products being build, tested and deployed with that
pipeline may require different workflows or processes in order for them to complete all the
stages in the pipeline. For example building and packaging a library will require a different
approach then building, testing and deploying a cloud service.
Additionally the different stages in the pipeline will require different approaches, e.g. build steps
will in general be executed by a build system returning the results in a synchronous way, however
test steps might run on a different machine from the process that controls the test steps so those
results might come back via an asynchronous route.
Finally flexibility is also required when dealing with resilience. In case of a disruption
flexibility may help with quickly restoring services through alternate means.

Making a flexible pipeline is achieved in the same way flexibility is achieved in other software
products, by using modular parts, standard inputs and outputs and carefully considered design. Some
of the appropriate options are for instance:

- Split the pipeline into stages that take standard inputs and deliver standard outputs. There might
  be many different types of inputs and outputs but they should be known and easily shared between
  processes and applications. There can be one or more stages, e.g. build, test and deploy, which
  are dependent on each other only through their inputs and outputs. This allows adding more stages
  if required.
- The steps which are executed inside a stage should be allowed to vary in number, type and order.
  As with the stages steps should ideally use well-known inputs and produce well-known outputs.
- Inputs for stages and steps are for instance
    - Source information, e.g. a commit ID
    - Artefacts, e.g. packages installers, zip files etc.
    - Meta data, additional information attached to a given output or input, e.g. build or test results
- Outputs generated by stages and steps are for instance
    - Artefacts, e.g. packages, installers, zip files etc.
    - Meta data, additional information to be attached to an artefact, e.g. test results

In order to make the most of the pipeline it helps if the artefacts generated in the pipeline
aren't created, tested and deployed in a single monolitic process. It is much simpler to use the
flexibility of the pipeline for a product that consists of many smaller components than it is to
do so for a product that consists of a large single component.



    - By  splitting the process into multiple stages it is p

- Implementation
    - Keep majority of the process described in the scripts. They're easier to adapt. Additionally
      keeping the process in the scripts means that developers can execute the majority of the pipeline
      from their local machines. That allows them to ensure builds / tests work before pushing to the
      pipeline, provides a means of building things if the pipeline is offline etc.
    - Any part of the process that cannot be done by a script, e.g. test systems, items that need services (e.g.
      certificate signing, which require that the certificates are present on the current machine, which is something
      you will probably not want to do on every machine) etc., should have a service that is available to both
      the build system and the developers executing the scripts locally. For any services that should only
      be provided to the build server (e.g. signing) the scripts should allow skipping the steps that
      need the service.
    - Provide DSL for describing a pipeline
    - Automatic generation of the pipeline job configuations, so that it is easy to update when changes occur
