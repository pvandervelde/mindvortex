Title: Software development pipeline - Design flexibilty
Tags:
  - Delivering software
  - Software development pipeline
  - Pipeline design
  - DevOps
---

The [fourth property](Software-development-pipeline-Design-introduction.html) to consider is
*flexibility*, i.e. the ability of the pipeline to be able to be modified or adapted without
requiring large changes to be made to the underlying pipeline code.

A pipeline should be flexible because the products being build, tested and deployed with that
pipeline may require different workflows or processes in order for them to complete all the
stages in the pipeline. For example building and packaging a library will require a different
approach then building, testing and deploying a cloud service.
Additionally the different stages in the pipeline will require different approaches, e.g. build steps
will in general be executed by a build system returning the results in a synchronous way, however
test steps might run on a different machine from the process that controls the test steps so those
results might come back via an asynchronous route.
Finally flexibility also improves resilience since in case of a disruption
an adaptable or flexible pipeline will allow restoring services through alternate means.

Making a flexible pipeline is achieved in the same way flexibility is achieved in other software
products, by using modular parts, standard inputs and outputs and carefully considered design. Some
of the appropriate options are for instance:

- Split the pipeline into stages that take standard inputs and deliver standard outputs. There might
  be many different types of inputs and outputs but they should be known and easily shared between
  processes and applications. There can be one or more stages, e.g. build, test and deploy, which
  are dependent on each other only through their inputs and outputs. This allows adding more stages
  if required.
- A step or stage in the pipeline can be started through a response to a standard notification. That
  allows each step to determine what information it needs to start exectution. Additional information
  can be downloaded from the appropriate sources upon receiving a notification. This approach allows
  notifications to be generic  while steps can still acquire the information they need to execute.
  Additionally having pipeline steps respond to notifications means that it is very easy to add new
  steps in the process because a new executor only has to be instantiated and connected to the
  message source, e.g. a distributed queue.
- If a stage consists of multiple, dependent steps, then it should be easy to add and remove
  steps based on the requirements. In these cases it would generally be preferred that a stage like
  this executes one or more scripts as they are easier to extend than services. As with the stages steps
  should ideally use well-known inputs and produce well-known outputs.
- Inputs for stages and steps are for instance
    - Source information, e.g. a commit ID
    - Artefacts, e.g. packages installers, zip files etc.
    - Meta data, additional information attached to a given output or input, e.g. build or test results
- Outputs generated by stages and steps are for instance
    - Artefacts, e.g. packages, installers, zip files etc.
    - Meta data, additional information to be attached to an artefact, e.g. test results

In order to make the most of the pipeline it is better if the artefacts generated in the pipeline
are not created, tested and deployed in a single monolitic process. It is much simpler to use the
flexibility of the pipeline for a product that consists of many smaller components than it is to
do so for a product that consists of a large single component because smaller components
can be created much quicker and in general assembly of a larger piece from components is quicker and
more flexible than regeneration of the entire piece from scratch. In many cases only a few components
will be recreated which saves time.

The exact implementation of the pipeline determines how flexible and easy to extend it will be. Given that
the use and implementation of the pipeline vary quite a lot it is hard to provide detailed implementation
details, however some basic suggestions are:

- Keep the build part of the pipeline described in the scripts given that scripts are, in general,
  easier to adapt. By pulling the scripts from a package, e.g. a NuGet or NPM package, it is in general
  quick and easy to update to a later version of these scripts. An additional benefit of
  keeping the process in the scripts is that developers can execute the individual steps of the pipeline
  from their local machines. That allows them to ensure builds / tests work before pushing to the
  pipeline and provides a means of building things if the pipeline is not available.
- It is easier to reason about a pipeline if the entire description of that pipeline is stored in
  a single file, ideally in source control. However as the pipeline evolves and more steps and stages
  are executed in parallel it will become increasingly difficult to capture the entire pipeline in
  a single file. While harder to reason about it is in the end
  simpler and more flexible to let the pipeline layout, as in the stages, steps and orders of these items,
  be determined by the executors that are available and listening for notifications. That way it's easy
  to change the layout of the pipeline.
- Any part of the process that cannot be done by a script, e.g. test systems, items that need services, e.g.
  certificate signing, which require that the certificates are present on the current machine, something
  which might not be possible to do on every machine etc., should have a service that is available to both
  the pipeline and the developers executing the scripts locally. For any services that should only
  be provided to the build server, e.g. signing, the scripts should allow skipping the steps that
  need the service.
- For stages that execute scripts, e.g. the build stage, jobs can be automatically generated
  from information stored in source control. This makes it easy to update the actions executed by these
  stages without requiring developers to perform the configuration manually.



NEED TO END THIS????
